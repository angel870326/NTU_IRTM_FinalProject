{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SVM linear, SVM RBF, Adaboost, XGboost（只用降維後純文本做預測）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 載入套件"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.svm import SVC\n",
    "from time import time\n",
    "from sklearn import metrics\n",
    "import scikitplot as skplt\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 讀檔"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "## stock sign\n",
    "filePath = \"./preprocess/tokens+stockSign/\"\n",
    "df_stockSign_covid = pd.read_csv(filePath + \"cvaw_covid_stockSign.csv\")[[\"date\", \"stockRise_mask\", \"stockRise_testKits\", \"stockRise_vaccine\"]]\n",
    "df_stockSign_stock = pd.read_csv(filePath + \"cvaw_stock_stockSign.csv\")[[\"date\", \"stockRise_mask\", \"stockRise_testKits\", \"stockRise_vaccine\"]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 函式：讀 tokens data，切成 x_train, x_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def SVD_embedding(tokens_name):\n",
    "    df_embedding = pd.read_csv(\"./TFIDF/\" + tokens_name + \"_stockSign_TFIDF_TruncatedSVD.csv\")\n",
    "    \n",
    "    df_embedding = df_embedding.iloc[:, 1:501]\n",
    "    \n",
    "    x_train, x_test = train_test_split(df_embedding, test_size=0.20, random_state=404)\n",
    "    \n",
    "    return x_train, x_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def word2vec_embedding(tokens_name):\n",
    "    df_embedding = pd.read_csv(\"./Word2Vec/\" + tokens_name + \"_stockSign_word2vec.csv\")\n",
    "    \n",
    "    df_embedding = df_embedding.iloc[:, 1:501]\n",
    "    \n",
    "    x_train, x_test = train_test_split(df_embedding, test_size=0.20, random_state=404)\n",
    "    \n",
    "    return x_train, x_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 口罩類股 斷詞 testing & training split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_mask, x_test_mask = word2vec_embedding(\"covid_token\")\n",
    "y_train_mask, y_test_mask = train_test_split(df_stockSign_covid[[\"stockRise_mask\"]], test_size=0.20, random_state=404)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>490</th>\n",
       "      <th>491</th>\n",
       "      <th>492</th>\n",
       "      <th>493</th>\n",
       "      <th>494</th>\n",
       "      <th>495</th>\n",
       "      <th>496</th>\n",
       "      <th>497</th>\n",
       "      <th>498</th>\n",
       "      <th>499</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>7271</th>\n",
       "      <td>0.466977</td>\n",
       "      <td>0.490552</td>\n",
       "      <td>-0.046918</td>\n",
       "      <td>-0.202188</td>\n",
       "      <td>0.166707</td>\n",
       "      <td>0.052072</td>\n",
       "      <td>0.100822</td>\n",
       "      <td>0.127350</td>\n",
       "      <td>0.293276</td>\n",
       "      <td>0.144377</td>\n",
       "      <td>...</td>\n",
       "      <td>0.046730</td>\n",
       "      <td>0.167719</td>\n",
       "      <td>-0.160782</td>\n",
       "      <td>0.080226</td>\n",
       "      <td>-0.125111</td>\n",
       "      <td>-0.088842</td>\n",
       "      <td>0.191329</td>\n",
       "      <td>-0.090425</td>\n",
       "      <td>0.034194</td>\n",
       "      <td>-0.061873</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>317</th>\n",
       "      <td>-0.137972</td>\n",
       "      <td>0.238994</td>\n",
       "      <td>-0.103087</td>\n",
       "      <td>-0.276188</td>\n",
       "      <td>-0.096632</td>\n",
       "      <td>0.055066</td>\n",
       "      <td>-0.054771</td>\n",
       "      <td>0.103535</td>\n",
       "      <td>-0.027499</td>\n",
       "      <td>0.190077</td>\n",
       "      <td>...</td>\n",
       "      <td>0.052938</td>\n",
       "      <td>0.030108</td>\n",
       "      <td>0.247392</td>\n",
       "      <td>-0.107904</td>\n",
       "      <td>-0.122517</td>\n",
       "      <td>-0.083494</td>\n",
       "      <td>0.085071</td>\n",
       "      <td>0.077354</td>\n",
       "      <td>0.072515</td>\n",
       "      <td>0.231514</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1925</th>\n",
       "      <td>-0.272390</td>\n",
       "      <td>-0.219359</td>\n",
       "      <td>0.030242</td>\n",
       "      <td>0.378799</td>\n",
       "      <td>0.014876</td>\n",
       "      <td>-0.015882</td>\n",
       "      <td>-0.218432</td>\n",
       "      <td>0.099289</td>\n",
       "      <td>0.055401</td>\n",
       "      <td>0.247769</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.082648</td>\n",
       "      <td>-0.125051</td>\n",
       "      <td>-0.139779</td>\n",
       "      <td>-0.022874</td>\n",
       "      <td>0.055691</td>\n",
       "      <td>-0.276878</td>\n",
       "      <td>0.057967</td>\n",
       "      <td>0.295714</td>\n",
       "      <td>-0.024754</td>\n",
       "      <td>0.151208</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3188</th>\n",
       "      <td>-0.165721</td>\n",
       "      <td>0.238822</td>\n",
       "      <td>-0.049381</td>\n",
       "      <td>-0.021363</td>\n",
       "      <td>0.317870</td>\n",
       "      <td>-0.113444</td>\n",
       "      <td>-0.313132</td>\n",
       "      <td>0.128785</td>\n",
       "      <td>0.005647</td>\n",
       "      <td>0.069155</td>\n",
       "      <td>...</td>\n",
       "      <td>0.219933</td>\n",
       "      <td>0.324596</td>\n",
       "      <td>-0.353152</td>\n",
       "      <td>0.234333</td>\n",
       "      <td>-0.128627</td>\n",
       "      <td>0.060161</td>\n",
       "      <td>0.052233</td>\n",
       "      <td>-0.132020</td>\n",
       "      <td>0.074416</td>\n",
       "      <td>0.239536</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7289</th>\n",
       "      <td>-0.231800</td>\n",
       "      <td>-0.203382</td>\n",
       "      <td>0.166360</td>\n",
       "      <td>-0.131830</td>\n",
       "      <td>0.190765</td>\n",
       "      <td>-0.032649</td>\n",
       "      <td>0.228955</td>\n",
       "      <td>-0.392035</td>\n",
       "      <td>-0.171504</td>\n",
       "      <td>0.106874</td>\n",
       "      <td>...</td>\n",
       "      <td>0.144791</td>\n",
       "      <td>0.023806</td>\n",
       "      <td>0.343031</td>\n",
       "      <td>-0.069726</td>\n",
       "      <td>-0.126730</td>\n",
       "      <td>0.170638</td>\n",
       "      <td>0.217938</td>\n",
       "      <td>0.494262</td>\n",
       "      <td>0.491883</td>\n",
       "      <td>0.135065</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6031</th>\n",
       "      <td>0.635976</td>\n",
       "      <td>0.370562</td>\n",
       "      <td>-0.194026</td>\n",
       "      <td>-0.273566</td>\n",
       "      <td>0.023036</td>\n",
       "      <td>0.088587</td>\n",
       "      <td>0.273419</td>\n",
       "      <td>0.192486</td>\n",
       "      <td>0.337552</td>\n",
       "      <td>0.173713</td>\n",
       "      <td>...</td>\n",
       "      <td>0.366876</td>\n",
       "      <td>-0.402000</td>\n",
       "      <td>-0.139996</td>\n",
       "      <td>-0.355655</td>\n",
       "      <td>-0.181885</td>\n",
       "      <td>-0.530296</td>\n",
       "      <td>-0.050131</td>\n",
       "      <td>0.032169</td>\n",
       "      <td>-0.168377</td>\n",
       "      <td>-0.005768</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5108</th>\n",
       "      <td>-0.282629</td>\n",
       "      <td>-0.084010</td>\n",
       "      <td>-0.158663</td>\n",
       "      <td>0.005067</td>\n",
       "      <td>-0.008301</td>\n",
       "      <td>-0.057799</td>\n",
       "      <td>-0.283823</td>\n",
       "      <td>-0.172922</td>\n",
       "      <td>-0.028213</td>\n",
       "      <td>0.138294</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.053598</td>\n",
       "      <td>-0.079068</td>\n",
       "      <td>0.208914</td>\n",
       "      <td>-0.076541</td>\n",
       "      <td>-0.203598</td>\n",
       "      <td>-0.086399</td>\n",
       "      <td>-0.062242</td>\n",
       "      <td>-0.176672</td>\n",
       "      <td>-0.209389</td>\n",
       "      <td>0.280768</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5302</th>\n",
       "      <td>-0.183439</td>\n",
       "      <td>-0.147837</td>\n",
       "      <td>-0.012782</td>\n",
       "      <td>0.516806</td>\n",
       "      <td>-0.024616</td>\n",
       "      <td>-0.445040</td>\n",
       "      <td>0.228963</td>\n",
       "      <td>-0.249978</td>\n",
       "      <td>-0.056679</td>\n",
       "      <td>0.074657</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.059065</td>\n",
       "      <td>0.019059</td>\n",
       "      <td>0.297540</td>\n",
       "      <td>-0.307480</td>\n",
       "      <td>-0.170445</td>\n",
       "      <td>-0.036629</td>\n",
       "      <td>-0.077072</td>\n",
       "      <td>0.211082</td>\n",
       "      <td>-0.150104</td>\n",
       "      <td>0.504225</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5994</th>\n",
       "      <td>0.219891</td>\n",
       "      <td>0.571944</td>\n",
       "      <td>0.013836</td>\n",
       "      <td>-0.294966</td>\n",
       "      <td>0.212134</td>\n",
       "      <td>0.060913</td>\n",
       "      <td>0.161956</td>\n",
       "      <td>-0.021638</td>\n",
       "      <td>0.250821</td>\n",
       "      <td>-0.284589</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.100324</td>\n",
       "      <td>-0.328688</td>\n",
       "      <td>-0.115157</td>\n",
       "      <td>0.037604</td>\n",
       "      <td>0.034764</td>\n",
       "      <td>-0.154887</td>\n",
       "      <td>0.087008</td>\n",
       "      <td>-0.282653</td>\n",
       "      <td>-0.279951</td>\n",
       "      <td>-0.046932</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5884</th>\n",
       "      <td>-0.271721</td>\n",
       "      <td>-0.119965</td>\n",
       "      <td>-0.022200</td>\n",
       "      <td>0.173570</td>\n",
       "      <td>-0.233823</td>\n",
       "      <td>-0.198618</td>\n",
       "      <td>0.107825</td>\n",
       "      <td>-0.133190</td>\n",
       "      <td>-0.060561</td>\n",
       "      <td>0.233703</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.067091</td>\n",
       "      <td>0.012858</td>\n",
       "      <td>-0.038522</td>\n",
       "      <td>-0.094421</td>\n",
       "      <td>-0.049920</td>\n",
       "      <td>-0.241872</td>\n",
       "      <td>0.105001</td>\n",
       "      <td>0.420913</td>\n",
       "      <td>0.038877</td>\n",
       "      <td>0.244107</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>6011 rows × 500 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             0         1         2         3         4         5         6  \\\n",
       "7271  0.466977  0.490552 -0.046918 -0.202188  0.166707  0.052072  0.100822   \n",
       "317  -0.137972  0.238994 -0.103087 -0.276188 -0.096632  0.055066 -0.054771   \n",
       "1925 -0.272390 -0.219359  0.030242  0.378799  0.014876 -0.015882 -0.218432   \n",
       "3188 -0.165721  0.238822 -0.049381 -0.021363  0.317870 -0.113444 -0.313132   \n",
       "7289 -0.231800 -0.203382  0.166360 -0.131830  0.190765 -0.032649  0.228955   \n",
       "...        ...       ...       ...       ...       ...       ...       ...   \n",
       "6031  0.635976  0.370562 -0.194026 -0.273566  0.023036  0.088587  0.273419   \n",
       "5108 -0.282629 -0.084010 -0.158663  0.005067 -0.008301 -0.057799 -0.283823   \n",
       "5302 -0.183439 -0.147837 -0.012782  0.516806 -0.024616 -0.445040  0.228963   \n",
       "5994  0.219891  0.571944  0.013836 -0.294966  0.212134  0.060913  0.161956   \n",
       "5884 -0.271721 -0.119965 -0.022200  0.173570 -0.233823 -0.198618  0.107825   \n",
       "\n",
       "             7         8         9  ...       490       491       492  \\\n",
       "7271  0.127350  0.293276  0.144377  ...  0.046730  0.167719 -0.160782   \n",
       "317   0.103535 -0.027499  0.190077  ...  0.052938  0.030108  0.247392   \n",
       "1925  0.099289  0.055401  0.247769  ... -0.082648 -0.125051 -0.139779   \n",
       "3188  0.128785  0.005647  0.069155  ...  0.219933  0.324596 -0.353152   \n",
       "7289 -0.392035 -0.171504  0.106874  ...  0.144791  0.023806  0.343031   \n",
       "...        ...       ...       ...  ...       ...       ...       ...   \n",
       "6031  0.192486  0.337552  0.173713  ...  0.366876 -0.402000 -0.139996   \n",
       "5108 -0.172922 -0.028213  0.138294  ... -0.053598 -0.079068  0.208914   \n",
       "5302 -0.249978 -0.056679  0.074657  ... -0.059065  0.019059  0.297540   \n",
       "5994 -0.021638  0.250821 -0.284589  ... -0.100324 -0.328688 -0.115157   \n",
       "5884 -0.133190 -0.060561  0.233703  ... -0.067091  0.012858 -0.038522   \n",
       "\n",
       "           493       494       495       496       497       498       499  \n",
       "7271  0.080226 -0.125111 -0.088842  0.191329 -0.090425  0.034194 -0.061873  \n",
       "317  -0.107904 -0.122517 -0.083494  0.085071  0.077354  0.072515  0.231514  \n",
       "1925 -0.022874  0.055691 -0.276878  0.057967  0.295714 -0.024754  0.151208  \n",
       "3188  0.234333 -0.128627  0.060161  0.052233 -0.132020  0.074416  0.239536  \n",
       "7289 -0.069726 -0.126730  0.170638  0.217938  0.494262  0.491883  0.135065  \n",
       "...        ...       ...       ...       ...       ...       ...       ...  \n",
       "6031 -0.355655 -0.181885 -0.530296 -0.050131  0.032169 -0.168377 -0.005768  \n",
       "5108 -0.076541 -0.203598 -0.086399 -0.062242 -0.176672 -0.209389  0.280768  \n",
       "5302 -0.307480 -0.170445 -0.036629 -0.077072  0.211082 -0.150104  0.504225  \n",
       "5994  0.037604  0.034764 -0.154887  0.087008 -0.282653 -0.279951 -0.046932  \n",
       "5884 -0.094421 -0.049920 -0.241872  0.105001  0.420913  0.038877  0.244107  \n",
       "\n",
       "[6011 rows x 500 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_mask = x_train_mask.values.tolist()\n",
    "x_test_mask = x_test_mask.values.tolist()\n",
    "y_train_mask = y_train_mask[\"stockRise_mask\"].tolist()\n",
    "y_test_mask = y_test_mask[\"stockRise_mask\"].tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 檢測試劑類股 斷詞 testing & training split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_testKits, x_test_testKits = SVD_embedding(\"stock_token\")\n",
    "y_train_testKits, y_test_testKits = train_test_split(df_stockSign_stock[[\"stockRise_testKits\"]], test_size=0.20, random_state=404)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>490</th>\n",
       "      <th>491</th>\n",
       "      <th>492</th>\n",
       "      <th>493</th>\n",
       "      <th>494</th>\n",
       "      <th>495</th>\n",
       "      <th>496</th>\n",
       "      <th>497</th>\n",
       "      <th>498</th>\n",
       "      <th>499</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>6004</th>\n",
       "      <td>0.110194</td>\n",
       "      <td>-0.056617</td>\n",
       "      <td>0.016188</td>\n",
       "      <td>0.051068</td>\n",
       "      <td>0.024915</td>\n",
       "      <td>-0.024274</td>\n",
       "      <td>-0.014723</td>\n",
       "      <td>-0.026258</td>\n",
       "      <td>-0.055791</td>\n",
       "      <td>-0.005208</td>\n",
       "      <td>...</td>\n",
       "      <td>0.006400</td>\n",
       "      <td>-0.009511</td>\n",
       "      <td>-0.008505</td>\n",
       "      <td>0.003994</td>\n",
       "      <td>0.017639</td>\n",
       "      <td>-0.015786</td>\n",
       "      <td>0.002318</td>\n",
       "      <td>0.026095</td>\n",
       "      <td>0.000526</td>\n",
       "      <td>0.005658</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6702</th>\n",
       "      <td>0.215222</td>\n",
       "      <td>-0.029807</td>\n",
       "      <td>-0.106440</td>\n",
       "      <td>-0.060623</td>\n",
       "      <td>-0.081690</td>\n",
       "      <td>-0.008828</td>\n",
       "      <td>-0.005497</td>\n",
       "      <td>0.125946</td>\n",
       "      <td>-0.049947</td>\n",
       "      <td>-0.023866</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000037</td>\n",
       "      <td>0.008915</td>\n",
       "      <td>0.006572</td>\n",
       "      <td>-0.016545</td>\n",
       "      <td>-0.014984</td>\n",
       "      <td>0.025858</td>\n",
       "      <td>-0.018918</td>\n",
       "      <td>-0.001705</td>\n",
       "      <td>0.016183</td>\n",
       "      <td>0.027335</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4971</th>\n",
       "      <td>0.190443</td>\n",
       "      <td>0.456539</td>\n",
       "      <td>-0.089589</td>\n",
       "      <td>0.113550</td>\n",
       "      <td>0.085209</td>\n",
       "      <td>-0.011482</td>\n",
       "      <td>-0.013740</td>\n",
       "      <td>0.040625</td>\n",
       "      <td>-0.076117</td>\n",
       "      <td>0.018417</td>\n",
       "      <td>...</td>\n",
       "      <td>0.045564</td>\n",
       "      <td>0.011609</td>\n",
       "      <td>0.001865</td>\n",
       "      <td>-0.018082</td>\n",
       "      <td>0.016909</td>\n",
       "      <td>0.012102</td>\n",
       "      <td>0.003540</td>\n",
       "      <td>-0.022592</td>\n",
       "      <td>0.000633</td>\n",
       "      <td>-0.007836</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1867</th>\n",
       "      <td>0.230840</td>\n",
       "      <td>-0.021076</td>\n",
       "      <td>-0.092870</td>\n",
       "      <td>-0.061819</td>\n",
       "      <td>-0.110322</td>\n",
       "      <td>-0.008187</td>\n",
       "      <td>0.002349</td>\n",
       "      <td>0.030462</td>\n",
       "      <td>0.027457</td>\n",
       "      <td>0.002368</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.004193</td>\n",
       "      <td>0.005546</td>\n",
       "      <td>0.020345</td>\n",
       "      <td>0.010719</td>\n",
       "      <td>-0.002002</td>\n",
       "      <td>0.002424</td>\n",
       "      <td>0.010414</td>\n",
       "      <td>-0.025819</td>\n",
       "      <td>0.006383</td>\n",
       "      <td>0.013708</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2484</th>\n",
       "      <td>0.153676</td>\n",
       "      <td>-0.052038</td>\n",
       "      <td>0.060150</td>\n",
       "      <td>0.017483</td>\n",
       "      <td>0.192235</td>\n",
       "      <td>0.037284</td>\n",
       "      <td>-0.009671</td>\n",
       "      <td>0.023961</td>\n",
       "      <td>0.167582</td>\n",
       "      <td>-0.376188</td>\n",
       "      <td>...</td>\n",
       "      <td>0.009907</td>\n",
       "      <td>0.001814</td>\n",
       "      <td>-0.013591</td>\n",
       "      <td>0.012238</td>\n",
       "      <td>-0.005709</td>\n",
       "      <td>0.024119</td>\n",
       "      <td>-0.035278</td>\n",
       "      <td>0.012393</td>\n",
       "      <td>-0.016911</td>\n",
       "      <td>0.011092</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6031</th>\n",
       "      <td>0.114674</td>\n",
       "      <td>0.005346</td>\n",
       "      <td>-0.026531</td>\n",
       "      <td>-0.063207</td>\n",
       "      <td>0.020567</td>\n",
       "      <td>0.034371</td>\n",
       "      <td>0.034781</td>\n",
       "      <td>-0.019504</td>\n",
       "      <td>-0.047736</td>\n",
       "      <td>0.029909</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.014838</td>\n",
       "      <td>-0.003390</td>\n",
       "      <td>0.016402</td>\n",
       "      <td>-0.007536</td>\n",
       "      <td>0.015456</td>\n",
       "      <td>0.012212</td>\n",
       "      <td>0.022440</td>\n",
       "      <td>0.019762</td>\n",
       "      <td>-0.006434</td>\n",
       "      <td>-0.001645</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5108</th>\n",
       "      <td>0.140593</td>\n",
       "      <td>-0.014271</td>\n",
       "      <td>-0.065399</td>\n",
       "      <td>-0.031681</td>\n",
       "      <td>-0.076528</td>\n",
       "      <td>-0.009261</td>\n",
       "      <td>0.001531</td>\n",
       "      <td>0.053130</td>\n",
       "      <td>-0.032995</td>\n",
       "      <td>-0.036033</td>\n",
       "      <td>...</td>\n",
       "      <td>0.006875</td>\n",
       "      <td>-0.007357</td>\n",
       "      <td>-0.017490</td>\n",
       "      <td>0.045533</td>\n",
       "      <td>0.011390</td>\n",
       "      <td>0.017935</td>\n",
       "      <td>0.011710</td>\n",
       "      <td>0.002624</td>\n",
       "      <td>-0.019156</td>\n",
       "      <td>-0.023624</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5302</th>\n",
       "      <td>0.080773</td>\n",
       "      <td>-0.011286</td>\n",
       "      <td>0.018688</td>\n",
       "      <td>0.016437</td>\n",
       "      <td>-0.017477</td>\n",
       "      <td>-0.009216</td>\n",
       "      <td>-0.000077</td>\n",
       "      <td>-0.032430</td>\n",
       "      <td>0.000492</td>\n",
       "      <td>0.047174</td>\n",
       "      <td>...</td>\n",
       "      <td>0.018001</td>\n",
       "      <td>-0.024973</td>\n",
       "      <td>-0.013130</td>\n",
       "      <td>0.004887</td>\n",
       "      <td>-0.012008</td>\n",
       "      <td>0.003088</td>\n",
       "      <td>0.024134</td>\n",
       "      <td>0.012363</td>\n",
       "      <td>-0.005537</td>\n",
       "      <td>0.018533</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5884</th>\n",
       "      <td>0.158278</td>\n",
       "      <td>0.128662</td>\n",
       "      <td>-0.078809</td>\n",
       "      <td>0.024062</td>\n",
       "      <td>0.000665</td>\n",
       "      <td>-0.018054</td>\n",
       "      <td>0.009116</td>\n",
       "      <td>0.010543</td>\n",
       "      <td>0.001331</td>\n",
       "      <td>-0.067082</td>\n",
       "      <td>...</td>\n",
       "      <td>0.005738</td>\n",
       "      <td>0.001595</td>\n",
       "      <td>-0.016617</td>\n",
       "      <td>0.008776</td>\n",
       "      <td>-0.010579</td>\n",
       "      <td>0.002256</td>\n",
       "      <td>-0.008569</td>\n",
       "      <td>0.012491</td>\n",
       "      <td>-0.017333</td>\n",
       "      <td>-0.002944</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7911</th>\n",
       "      <td>0.089186</td>\n",
       "      <td>-0.013863</td>\n",
       "      <td>0.013187</td>\n",
       "      <td>0.007128</td>\n",
       "      <td>-0.007844</td>\n",
       "      <td>-0.026448</td>\n",
       "      <td>0.001017</td>\n",
       "      <td>-0.009380</td>\n",
       "      <td>-0.002694</td>\n",
       "      <td>-0.030506</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.001294</td>\n",
       "      <td>0.001632</td>\n",
       "      <td>-0.024623</td>\n",
       "      <td>0.016993</td>\n",
       "      <td>0.003443</td>\n",
       "      <td>-0.010593</td>\n",
       "      <td>0.000378</td>\n",
       "      <td>0.005904</td>\n",
       "      <td>-0.013806</td>\n",
       "      <td>0.009312</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>7783 rows × 500 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             0         1         2         3         4         5         6  \\\n",
       "6004  0.110194 -0.056617  0.016188  0.051068  0.024915 -0.024274 -0.014723   \n",
       "6702  0.215222 -0.029807 -0.106440 -0.060623 -0.081690 -0.008828 -0.005497   \n",
       "4971  0.190443  0.456539 -0.089589  0.113550  0.085209 -0.011482 -0.013740   \n",
       "1867  0.230840 -0.021076 -0.092870 -0.061819 -0.110322 -0.008187  0.002349   \n",
       "2484  0.153676 -0.052038  0.060150  0.017483  0.192235  0.037284 -0.009671   \n",
       "...        ...       ...       ...       ...       ...       ...       ...   \n",
       "6031  0.114674  0.005346 -0.026531 -0.063207  0.020567  0.034371  0.034781   \n",
       "5108  0.140593 -0.014271 -0.065399 -0.031681 -0.076528 -0.009261  0.001531   \n",
       "5302  0.080773 -0.011286  0.018688  0.016437 -0.017477 -0.009216 -0.000077   \n",
       "5884  0.158278  0.128662 -0.078809  0.024062  0.000665 -0.018054  0.009116   \n",
       "7911  0.089186 -0.013863  0.013187  0.007128 -0.007844 -0.026448  0.001017   \n",
       "\n",
       "             7         8         9  ...       490       491       492  \\\n",
       "6004 -0.026258 -0.055791 -0.005208  ...  0.006400 -0.009511 -0.008505   \n",
       "6702  0.125946 -0.049947 -0.023866  ...  0.000037  0.008915  0.006572   \n",
       "4971  0.040625 -0.076117  0.018417  ...  0.045564  0.011609  0.001865   \n",
       "1867  0.030462  0.027457  0.002368  ... -0.004193  0.005546  0.020345   \n",
       "2484  0.023961  0.167582 -0.376188  ...  0.009907  0.001814 -0.013591   \n",
       "...        ...       ...       ...  ...       ...       ...       ...   \n",
       "6031 -0.019504 -0.047736  0.029909  ... -0.014838 -0.003390  0.016402   \n",
       "5108  0.053130 -0.032995 -0.036033  ...  0.006875 -0.007357 -0.017490   \n",
       "5302 -0.032430  0.000492  0.047174  ...  0.018001 -0.024973 -0.013130   \n",
       "5884  0.010543  0.001331 -0.067082  ...  0.005738  0.001595 -0.016617   \n",
       "7911 -0.009380 -0.002694 -0.030506  ... -0.001294  0.001632 -0.024623   \n",
       "\n",
       "           493       494       495       496       497       498       499  \n",
       "6004  0.003994  0.017639 -0.015786  0.002318  0.026095  0.000526  0.005658  \n",
       "6702 -0.016545 -0.014984  0.025858 -0.018918 -0.001705  0.016183  0.027335  \n",
       "4971 -0.018082  0.016909  0.012102  0.003540 -0.022592  0.000633 -0.007836  \n",
       "1867  0.010719 -0.002002  0.002424  0.010414 -0.025819  0.006383  0.013708  \n",
       "2484  0.012238 -0.005709  0.024119 -0.035278  0.012393 -0.016911  0.011092  \n",
       "...        ...       ...       ...       ...       ...       ...       ...  \n",
       "6031 -0.007536  0.015456  0.012212  0.022440  0.019762 -0.006434 -0.001645  \n",
       "5108  0.045533  0.011390  0.017935  0.011710  0.002624 -0.019156 -0.023624  \n",
       "5302  0.004887 -0.012008  0.003088  0.024134  0.012363 -0.005537  0.018533  \n",
       "5884  0.008776 -0.010579  0.002256 -0.008569  0.012491 -0.017333 -0.002944  \n",
       "7911  0.016993  0.003443 -0.010593  0.000378  0.005904 -0.013806  0.009312  \n",
       "\n",
       "[7783 rows x 500 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train_testKits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_testKits = x_train_testKits.values.tolist()\n",
    "x_test_testKits = x_test_testKits.values.tolist()\n",
    "y_train_testKits = y_train_testKits[\"stockRise_testKits\"].tolist()\n",
    "y_test_testKits = y_test_testKits[\"stockRise_testKits\"].tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 疫苗類股 斷詞 testing & training split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_vaccine, x_test_vaccine = SVD_embedding(\"stock_token\")\n",
    "y_train_vaccine, y_test_vaccine = train_test_split(df_stockSign_stock[[\"stockRise_vaccine\"]], test_size=0.20, random_state=404)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_vaccine = x_train_vaccine.values.tolist()\n",
    "x_test_vaccine = x_test_vaccine.values.tolist()\n",
    "y_train_vaccine = y_train_vaccine[\"stockRise_vaccine\"].tolist()\n",
    "y_test_vaccine = y_test_vaccine[\"stockRise_vaccine\"].tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SVM Model 函式"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokens_predict_stocks(x_train, y_train, x_test, y_test, tokens_name, predict_stocks_name, Linear, embedding):\n",
    "    \n",
    "    if Linear == True:\n",
    "        SVM_model = SVC(kernel = \"linear\", C = 1.0, probability=True)\n",
    "    else:\n",
    "        SVM_model = SVC(kernel = \"rbf\", gamma = \"scale\", C = 1.0, probability=True)\n",
    "    \n",
    "    t0 = time()\n",
    "    SVM_model.fit(x_train, y_train)\n",
    "    print(\"done in %0.3fs.\" % (time() - t0))\n",
    "    \n",
    "    predicted_results = []\n",
    "    excepted_results = []\n",
    "\n",
    "    excepted_results.extend(y_test)\n",
    "    predicted_results.extend(SVM_model.predict(x_test))\n",
    "    \n",
    "    if Linear == True:\n",
    "        print(tokens_name + \" predict \" + predict_stocks_name + \"(SVM Linear + \" + embedding + \")\")\n",
    "    else:\n",
    "        print(tokens_name + \" predict \" + predict_stocks_name + \"(SVM RBF + \" + embedding + \")\")\n",
    "        \n",
    "    print(metrics.classification_report(excepted_results,predicted_results))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### adaboost model 函式"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def adaboost_tokens_predict_stocks(x_train, y_train, x_test, y_test, tokens_name, predict_stocks_name, embedding, n_estimator):\n",
    "    \n",
    "    adaboost_model = AdaBoostClassifier(n_estimators = n_estimator, random_state = 404)\n",
    "    \n",
    "    t0 = time()\n",
    "    adaboost_model.fit(x_train, y_train)\n",
    "    print(\"done in %0.3fs.\" % (time() - t0))\n",
    "    \n",
    "    predicted_results = []\n",
    "    excepted_results = []\n",
    "\n",
    "    excepted_results.extend(y_test)\n",
    "    predicted_results.extend(adaboost_model.predict(x_test))\n",
    "\n",
    "    print(tokens_name + \" predict \" + predict_stocks_name + \"(adaboost + \" + embedding + \")\")\n",
    "        \n",
    "    print(metrics.classification_report(excepted_results,predicted_results))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### xgboost model 函式（train, test 不能轉為 list）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def xgboost_tokens_predict_stocks(x_train, y_train, x_test, y_test, tokens_name, predict_stocks_name, embedding, stock_type):\n",
    "    \n",
    "    y_test = y_test[\"stockRise_\" + stock_type].tolist()\n",
    "    \n",
    "    xgboost_model = XGBClassifier(random_state = 404)\n",
    "    \n",
    "    t0 = time()\n",
    "    xgboost_model.fit(x_train, y_train)\n",
    "    print(\"done in %0.3fs.\" % (time() - t0))\n",
    "    \n",
    "    predicted_results = []\n",
    "    excepted_results = []\n",
    "\n",
    "    excepted_results.extend(y_test)\n",
    "    predicted_results.extend(xgboost_model.predict(x_test))\n",
    "\n",
    "    print(tokens_name + \" predict \" + predict_stocks_name + \"(xgboost + \" + embedding + \")\")\n",
    "        \n",
    "    print(metrics.classification_report(excepted_results,predicted_results))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random forest 函式"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def randomForest_tokens_predict_stocks(x_train, y_train, x_test, y_test, tokens_name, predict_stocks_name, embedding):\n",
    "    \n",
    "    randomForest_model = RandomForestClassifier(n_estimators=500, max_features=50, random_state=404)\n",
    "    \n",
    "    t0 = time()\n",
    "    randomForest_model.fit(x_train, y_train)\n",
    "    print(\"done in %0.3fs.\" % (time() - t0))\n",
    "    \n",
    "    predicted_results = []\n",
    "    excepted_results = []\n",
    "\n",
    "    excepted_results.extend(y_test)\n",
    "    predicted_results.extend(randomForest_model.predict(x_test))\n",
    "\n",
    "    print(tokens_name + \" predict \" + predict_stocks_name + \"(random forest + \" + embedding + \")\")\n",
    "        \n",
    "    print(metrics.classification_report(excepted_results,predicted_results))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 預測口罩類股漲跌效果（word2vec covid 板 ckip 斷詞）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done in 171.444s.\n",
      "Covid CKIP token predict mask stock(SVM Linear + word2vec)\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          -1       0.40      0.49      0.44       557\n",
      "           0       0.00      0.00      0.00       324\n",
      "           1       0.44      0.58      0.50       622\n",
      "\n",
      "    accuracy                           0.42      1503\n",
      "   macro avg       0.28      0.36      0.31      1503\n",
      "weighted avg       0.33      0.42      0.37      1503\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\molly\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1221: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "tokens_predict_stocks(x_train_mask, y_train_mask, x_test_mask, y_test_mask, \"Covid CKIP token\", \"mask stock\", True, \"word2vec\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done in 148.992s.\n",
      "Covid CKIP token predict mask stock(SVM RBF + word2vec)\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          -1       0.41      0.54      0.46       557\n",
      "           0       0.29      0.01      0.01       324\n",
      "           1       0.44      0.54      0.49       622\n",
      "\n",
      "    accuracy                           0.42      1503\n",
      "   macro avg       0.38      0.36      0.32      1503\n",
      "weighted avg       0.40      0.42      0.38      1503\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tokens_predict_stocks(x_train_mask, y_train_mask, x_test_mask, y_test_mask, \"Covid CKIP token\", \"mask stock\", False, \"word2vec\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done in 36.189s.\n",
      "Covid CKIP token predict mask stock(adaboost + word2vec)\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          -1       0.37      0.44      0.40       557\n",
      "           0       0.30      0.17      0.22       324\n",
      "           1       0.41      0.43      0.42       622\n",
      "\n",
      "    accuracy                           0.38      1503\n",
      "   macro avg       0.36      0.35      0.35      1503\n",
      "weighted avg       0.37      0.38      0.37      1503\n",
      "\n"
     ]
    }
   ],
   "source": [
    "adaboost_tokens_predict_stocks(x_train_mask, y_train_mask, x_test_mask, y_test_mask, \"Covid CKIP token\", \"mask stock\", \"word2vec\", 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\molly\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n",
      "C:\\Users\\molly\\anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:72: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  return f(**kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[15:35:27] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "done in 51.570s.\n",
      "Covid CKIP token predict mask stock(xgboost + word2vec)\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          -1       0.40      0.45      0.42       557\n",
      "           0       0.26      0.13      0.17       324\n",
      "           1       0.44      0.49      0.46       622\n",
      "\n",
      "    accuracy                           0.40      1503\n",
      "   macro avg       0.36      0.36      0.35      1503\n",
      "weighted avg       0.38      0.40      0.38      1503\n",
      "\n"
     ]
    }
   ],
   "source": [
    "xgboost_tokens_predict_stocks(x_train_mask, y_train_mask, x_test_mask, y_test_mask, \"Covid CKIP token\", \"mask stock\", \"word2vec\", \"mask\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done in 113.179s.\n",
      "Covid CKIP token predict mask stock(random forest + word2vec)\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          -1       0.39      0.46      0.42       557\n",
      "           0       0.28      0.05      0.08       324\n",
      "           1       0.44      0.56      0.49       622\n",
      "\n",
      "    accuracy                           0.41      1503\n",
      "   macro avg       0.37      0.36      0.33      1503\n",
      "weighted avg       0.39      0.41      0.38      1503\n",
      "\n"
     ]
    }
   ],
   "source": [
    "randomForest_tokens_predict_stocks(x_train_mask, y_train_mask, x_test_mask, y_test_mask, \"Covid CKIP token\", \"mask stock\", \"word2vec\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 預測檢測試劑類股漲跌效果（SVD stock 板 jieba 斷詞）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done in 223.275s.\n",
      "Stock jieba token predict testKits stock(SVM Linear + SVD vector)\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          -1       0.44      0.06      0.10       525\n",
      "           0       0.45      0.95      0.61       855\n",
      "           1       0.33      0.03      0.06       566\n",
      "\n",
      "    accuracy                           0.44      1946\n",
      "   macro avg       0.40      0.35      0.26      1946\n",
      "weighted avg       0.41      0.44      0.31      1946\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tokens_predict_stocks(x_train_testKits, y_train_testKits, x_test_testKits, y_test_testKits, \"Stock jieba token\", \"testKits stock\", True, \"SVD vector\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done in 243.376s.\n",
      "Stock jieba token predict testKits stock(SVM RBF + SVD vector)\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          -1       0.40      0.16      0.23       525\n",
      "           0       0.46      0.81      0.59       855\n",
      "           1       0.36      0.14      0.20       566\n",
      "\n",
      "    accuracy                           0.44      1946\n",
      "   macro avg       0.40      0.37      0.34      1946\n",
      "weighted avg       0.41      0.44      0.38      1946\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tokens_predict_stocks(x_train_testKits, y_train_testKits, x_test_testKits, y_test_testKits, \"Stock jieba token\", \"testKits stock\", False, \"SVD vector\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done in 45.738s.\n",
      "Stock jieba token predict testKits stock(adaboost + SVD vector)\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          -1       0.30      0.24      0.27       525\n",
      "           0       0.46      0.63      0.53       855\n",
      "           1       0.31      0.21      0.25       566\n",
      "\n",
      "    accuracy                           0.40      1946\n",
      "   macro avg       0.36      0.36      0.35      1946\n",
      "weighted avg       0.38      0.40      0.38      1946\n",
      "\n"
     ]
    }
   ],
   "source": [
    "adaboost_tokens_predict_stocks(x_train_testKits, y_train_testKits, x_test_testKits, y_test_testKits, \"Stock jieba token\", \"testKits stock\", \"SVD vector\", 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[15:36:18] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "done in 81.634s.\n",
      "Stock jieba token predict testKits stock(xgboost + SVD vector)\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          -1       0.31      0.20      0.24       525\n",
      "           0       0.45      0.64      0.53       855\n",
      "           1       0.33      0.23      0.27       566\n",
      "\n",
      "    accuracy                           0.40      1946\n",
      "   macro avg       0.36      0.36      0.35      1946\n",
      "weighted avg       0.38      0.40      0.38      1946\n",
      "\n"
     ]
    }
   ],
   "source": [
    "xgboost_tokens_predict_stocks(x_train_testKits, y_train_testKits, x_test_testKits, y_test_testKits, \"Stock jieba token\", \"testKits stock\", \"SVD vector\", \"testKits\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done in 159.724s.\n",
      "Stock jieba token predict testKits stock(random forest + SVD vector)\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          -1       0.42      0.05      0.09       525\n",
      "           0       0.44      0.95      0.60       855\n",
      "           1       0.31      0.03      0.06       566\n",
      "\n",
      "    accuracy                           0.44      1946\n",
      "   macro avg       0.39      0.34      0.25      1946\n",
      "weighted avg       0.40      0.44      0.31      1946\n",
      "\n"
     ]
    }
   ],
   "source": [
    "randomForest_tokens_predict_stocks(x_train_testKits, y_train_testKits, x_test_testKits, y_test_testKits, \"Stock jieba token\", \"testKits stock\", \"SVD vector\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 預測疫苗類股漲跌效果（SVD stock 板 jieba 斷詞）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done in 197.480s.\n",
      "Stock jieba token predict vaccine stock(SVM Linear + SVD vector)\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          -1       0.12      0.00      0.01       428\n",
      "           0       0.56      0.99      0.72      1082\n",
      "           1       0.75      0.01      0.01       436\n",
      "\n",
      "    accuracy                           0.56      1946\n",
      "   macro avg       0.48      0.34      0.25      1946\n",
      "weighted avg       0.51      0.56      0.40      1946\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tokens_predict_stocks(x_train_vaccine, y_train_vaccine, x_test_vaccine, y_test_vaccine, \"Stock jieba token\", \"vaccine stock\", True, \"SVD vector\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done in 246.035s.\n",
      "Stock jieba token predict vaccine stock(SVM RBF + SVD vector)\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          -1       0.35      0.05      0.08       428\n",
      "           0       0.57      0.98      0.72      1082\n",
      "           1       0.34      0.03      0.06       436\n",
      "\n",
      "    accuracy                           0.56      1946\n",
      "   macro avg       0.42      0.35      0.29      1946\n",
      "weighted avg       0.47      0.56      0.43      1946\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tokens_predict_stocks(x_train_vaccine, y_train_vaccine, x_test_vaccine, y_test_vaccine, \"Stock jieba token\", \"vaccine stock\", False, \"SVD vector\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done in 42.233s.\n",
      "Stock jieba token predict vaccine stock(adaboost + SVD vector)\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          -1       0.23      0.08      0.12       428\n",
      "           0       0.57      0.86      0.69      1082\n",
      "           1       0.25      0.09      0.13       436\n",
      "\n",
      "    accuracy                           0.52      1946\n",
      "   macro avg       0.35      0.34      0.31      1946\n",
      "weighted avg       0.42      0.52      0.44      1946\n",
      "\n"
     ]
    }
   ],
   "source": [
    "adaboost_tokens_predict_stocks(x_train_vaccine, y_train_vaccine, x_test_vaccine, y_test_vaccine, \"Stock jieba token\", \"vaccine stock\", \"SVD vector\", 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[15:37:40] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "done in 84.425s.\n",
      "Stock jieba token predict vaccine stock(xgboost + SVD vector)\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          -1       0.36      0.12      0.18       428\n",
      "           0       0.59      0.91      0.72      1082\n",
      "           1       0.34      0.10      0.16       436\n",
      "\n",
      "    accuracy                           0.56      1946\n",
      "   macro avg       0.43      0.38      0.35      1946\n",
      "weighted avg       0.48      0.56      0.47      1946\n",
      "\n"
     ]
    }
   ],
   "source": [
    "xgboost_tokens_predict_stocks(x_train_vaccine, y_train_vaccine, x_test_vaccine, y_test_vaccine, \"Stock jieba token\", \"vaccine stock\", \"SVD vector\", \"vaccine\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done in 157.701s.\n",
      "Stock jieba token predict vaccine stock(random forest + SVD vector)\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          -1       0.47      0.02      0.04       428\n",
      "           0       0.56      1.00      0.72      1082\n",
      "           1       0.60      0.02      0.04       436\n",
      "\n",
      "    accuracy                           0.56      1946\n",
      "   macro avg       0.55      0.35      0.27      1946\n",
      "weighted avg       0.55      0.56      0.42      1946\n",
      "\n"
     ]
    }
   ],
   "source": [
    "randomForest_tokens_predict_stocks(x_train_vaccine, y_train_vaccine, x_test_vaccine, y_test_vaccine, \"Stock jieba token\", \"vaccine stock\", \"SVD vector\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
